---
title: "Modeling Barrel Classification in Big Ten College Baseball"
author: "Kyle Gilbert"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: pdflatex
    number_sections: true
---

```{r dependencies}

#| echo: false
#| message: false
#| warning: false

# Install pacman package
if (!require("pacman")) install.packages("pacman")

# Load all packages, installing if necessary
pacman::p_load(dplyr, mgcv, ggplot2, pROC, rsample, purrr, stats, ranger, utils)
```

```{r load-data}

#| echo: false
#| message: false
#| warning: false
#| output: false

# Read in Big 10 batted-ball data
data <- read.csv("data.csv")
```

```{r preprocessing}

#| echo: false
#| warning: false
#| message: false

# Compute exit velocity mean & sd
meanEV <- mean(df$EV, na.rm = TRUE)
sdEV <- sd(df$EV, na.rm = TRUE)

data <- data %>%
  mutate(
    Hit = PlayResult %in% c("Single", "Double", "Triple", "HomeRun"),
    Bases = case_when(
      PlayResult == "Single"  ~ 1,
      PlayResult == "Double"  ~ 2,
      PlayResult == "Triple"  ~ 3,
      PlayResult == "HomeRun" ~ 4,
      TRUE ~ 0
    )
  ) %>% 
  filter(
    EV >= (meanEV - 2 * sdEV), # ~2.5th percentile
    EV <= (meanEV + 2 * sdEV)  # ~97.5th percentile
  )
```

```{r model-evaluation-function}

#| echo: false
#| warning: false
#| message: false

# Evaluate model performance for regression or classification tasks
evaluate_model <- function(actual, predicted) {
  
  # Determine if regression or classification
  isRegression <- is.numeric(actual) && length(unique(actual)) > 2
  
  if(isRegression) {
    
    # Regression metrics
    tibble(
      mae = mean(abs(actual - predicted), na.rm = TRUE),
      rmse = sqrt(mean((actual - predicted)^2, na.rm = TRUE)),
      correlation = cor(actual, predicted, use = "complete.obs"),
      rSquared = correlation^2
    )
    
  } else {
    
    # Thresholding (probability -> binary class labels)
    predictedClass = ifelse(predicted >= 0.5, 1, 0)
    
    # Confusion matrix
    tp = sum(predictedClass == 1 & actual == 1) # True positive
    fp = sum(predictedClass == 1 & actual == 0) # False positive
    tn = sum(predictedClass == 0 & actual == 0) # True negative
    fn = sum(predictedClass == 0 & actual == 1) # False negative
    
    # Compute performance metrics
    accuracy = (tp + tn) / (tp + fp + tn + fn)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    aucVal = auc(actual, predicted)
    f1 = ifelse(
      is.na(precision) | is.na(recall) | (precision + recall) == 0,
      NA, 2 * (precision * recall) / (precision + recall)
    )
    
    # Classification metrics
    tibble(
      accuracy = accuracy,
      precision = precision,
      recall = recall,
      f1 = f1,
      auc = as.numeric(aucVal)
    )
  }
}
```

```{r cross-validation-function}

#| echo: false
#| warning: false
#| message: false

# Perform k-fold cross-validation and return performance metrics
cross_validate <- function(data, k, modelFn, responseVar) {
  
  # Stratified k-fold splits
  folds <- vfold_cv(data, v = k, strata = responseVar)
  
  # Evaluate model on each fold
  results <- map_dfr(folds$splits, function(fold) {
    
    # Derive train-test splits
    train <- analysis(fold)
    test <- assessment(fold)
    
    # Fit model on training data
    model <- modelFn(train)
    
    # Generate predictions on test data
    predictions <- predict(model, newdata = test, type = "response")
    
    # Extract actual values
    actual <- test[[responseVar]]
    
    # Evaluate prediction performance
    evaluate_model(actual, predictions)
  })
  
  return(results)
}
```

